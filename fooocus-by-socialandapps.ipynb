{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installation","metadata":{}},{"cell_type":"code","source":"HOME_FOLDER = '/kaggle/working'\nTEMP_FOLDER = '/kaggle/temp'\nupdate = False\n\ncheckpoints = f'{HOME_FOLDER}/Fooocus/models/checkpoints'\ntemp_models = f'{HOME_FOLDER}/Fooocus/models/checkpoints/temp_models'\n\n!mkdir $TEMP_FOLDER\n\nimport os\nfrom os import path\n\n%cd $HOME_FOLDER\n\nif not path.exists('Fooocus'):\n    get_ipython().system('git clone https://github.com/lllyasviel/Fooocus.git')\nelse:\n    get_ipython().system(f'find {HOME_FOLDER}/Fooocus/models/checkpoints -maxdepth 1 -type l -delete') # delete any symlinks to temp models from a previous run\n    \n%cd Fooocus\n\nif update:\n    get_ipython().system('git pull')\n    \n!pip install -r requirements_versions.txt \n!pip install torch torchvision --force-reinstall --index-url https://download.pytorch.org/whl/cu117\n!rm -rf /opt/conda/lib/python3.10/site-packages/pytz-2023.3.dist-info\n!rm -rf /opt/conda/lib/python3.10/site-packages/PyYAML-6.0.1.dist-info\n!pip install pyyaml pytz \n\n!mamba install openssh -y","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-04T10:44:34.397469Z","iopub.execute_input":"2024-01-04T10:44:34.398235Z","iopub.status.idle":"2024-01-04T10:49:18.699925Z","shell.execute_reply.started":"2024-01-04T10:44:34.398199Z","shell.execute_reply":"2024-01-04T10:49:18.698921Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/working\nCloning into 'Fooocus'...\nremote: Enumerating objects: 4914, done.\u001b[K\nremote: Counting objects: 100% (1338/1338), done.\u001b[K\nremote: Compressing objects: 100% (115/115), done.\u001b[K\nremote: Total 4914 (delta 1259), reused 1227 (delta 1222), pack-reused 3576\u001b[K\nReceiving objects: 100% (4914/4914), 24.86 MiB | 33.37 MiB/s, done.\nResolving deltas: 100% (3016/3016), done.\n/kaggle/working/Fooocus\nCollecting torchsde==0.2.5 (from -r requirements_versions.txt (line 1))\n  Downloading torchsde-0.2.5-py3-none-any.whl (59 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m947.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting einops==0.4.1 (from -r requirements_versions.txt (line 2))\n  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\nCollecting transformers==4.30.2 (from -r requirements_versions.txt (line 3))\n  Obtaining dependency information for transformers==4.30.2 from https://files.pythonhosted.org/packages/5b/0b/e45d26ccd28568013523e04f325432ea88a442b4e3020b757cf4361f0120/transformers-4.30.2-py3-none-any.whl.metadata\n  Downloading transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting safetensors==0.3.1 (from -r requirements_versions.txt (line 4))\n  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting accelerate==0.21.0 (from -r requirements_versions.txt (line 5))\n  Obtaining dependency information for accelerate==0.21.0 from https://files.pythonhosted.org/packages/70/f9/c381bcdd0c3829d723aa14eec8e75c6c377b4ca61ec68b8093d9f35fc7a7/accelerate-0.21.0-py3-none-any.whl.metadata\n  Downloading accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\nCollecting pyyaml==6.0 (from -r requirements_versions.txt (line 6))\n  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting Pillow==9.2.0 (from -r requirements_versions.txt (line 7))\n  Downloading Pillow-9.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting scipy==1.9.3 (from -r requirements_versions.txt (line 8))\n  Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.7/33.7 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting tqdm==4.64.1 (from -r requirements_versions.txt (line 9))\n  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting psutil==5.9.5 (from -r requirements_versions.txt (line 10))\n  Downloading psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.1/282.1 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pytorch_lightning==1.9.4 (from -r requirements_versions.txt (line 11))\n  Downloading pytorch_lightning-1.9.4-py3-none-any.whl (827 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m827.8/827.8 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting omegaconf==2.2.3 (from -r requirements_versions.txt (line 12))\n  Downloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting gradio==3.41.2 (from -r requirements_versions.txt (line 13))\n  Obtaining dependency information for gradio==3.41.2 from https://files.pythonhosted.org/packages/df/e7/e0b548208ff5db6323ad974f094e9435adb0a377f35274196fb74adaf58a/gradio-3.41.2-py3-none-any.whl.metadata\n  Downloading gradio-3.41.2-py3-none-any.whl.metadata (17 kB)\nCollecting pygit2==1.12.2 (from -r requirements_versions.txt (line 14))\n  Obtaining dependency information for pygit2==1.12.2 from https://files.pythonhosted.org/packages/33/20/ae3efebb6ce3e1a226a8f29bf6ed6bb9b89a79f38b9b833284185baac7ca/pygit2-1.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading pygit2-1.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\nCollecting opencv-contrib-python==4.8.0.74 (from -r requirements_versions.txt (line 15))\n  Obtaining dependency information for opencv-contrib-python==4.8.0.74 from https://files.pythonhosted.org/packages/d6/ca/f109e0a8f33074f0f74cf2677ee0b9bdc025d7fc07b2280afdf7fad38b47/opencv_contrib_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading opencv_contrib_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\nCollecting httpx==0.24.1 (from -r requirements_versions.txt (line 16))\n  Obtaining dependency information for httpx==0.24.1 from https://files.pythonhosted.org/packages/ec/91/e41f64f03d2a13aee7e8c819d82ee3aa7cdc484d18c0ae859742597d5aa0/httpx-0.24.1-py3-none-any.whl.metadata\n  Downloading httpx-0.24.1-py3-none-any.whl.metadata (7.4 kB)\nCollecting onnxruntime==1.16.3 (from -r requirements_versions.txt (line 17))\n  Obtaining dependency information for onnxruntime==1.16.3 from https://files.pythonhosted.org/packages/7a/cf/6aa8c56fd63f53c2c485921e411269c7b501a2b4e634bd02f226ab2d5d8e/onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\nCollecting timm==0.9.2 (from -r requirements_versions.txt (line 18))\n  Obtaining dependency information for timm==0.9.2 from https://files.pythonhosted.org/packages/29/90/94f5deb8d76e24a89813aef95e8809ca8fd7414490428480eda19b133d4a/timm-0.9.2-py3-none-any.whl.metadata\n  Downloading timm-0.9.2-py3-none-any.whl.metadata (68 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: boltons>=20.2.1 in /opt/conda/lib/python3.10/site-packages (from torchsde==0.2.5->-r requirements_versions.txt (line 1)) (23.0.0)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from torchsde==0.2.5->-r requirements_versions.txt (line 1)) (2.0.0)\nCollecting trampoline>=0.1.2 (from torchsde==0.2.5->-r requirements_versions.txt (line 1))\n  Downloading trampoline-0.1.2-py3-none-any.whl (5.2 kB)\nRequirement already satisfied: numpy>=1.19.* in /opt/conda/lib/python3.10/site-packages (from torchsde==0.2.5->-r requirements_versions.txt (line 1)) (1.24.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements_versions.txt (line 3)) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements_versions.txt (line 3)) (0.19.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements_versions.txt (line 3)) (21.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements_versions.txt (line 3)) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.2->-r requirements_versions.txt (line 3)) (2.31.0)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.2->-r requirements_versions.txt (line 3))\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: fsspec[http]>2021.06.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning==1.9.4->-r requirements_versions.txt (line 11)) (2023.12.2)\nRequirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning==1.9.4->-r requirements_versions.txt (line 11)) (1.2.1)\nRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning==1.9.4->-r requirements_versions.txt (line 11)) (4.5.0)\nRequirement already satisfied: lightning-utilities>=0.6.0.post0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning==1.9.4->-r requirements_versions.txt (line 11)) (0.10.0)\nCollecting antlr4-python3-runtime==4.9.* (from omegaconf==2.2.3->-r requirements_versions.txt (line 12))\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio==3.41.2->-r requirements_versions.txt (line 13))\n  Obtaining dependency information for aiofiles<24.0,>=22.0 from https://files.pythonhosted.org/packages/c5/19/5af6804c4cc0fed83f47bff6e413a98a36618e7d40185cd36e69737f3b0e/aiofiles-23.2.1-py3-none-any.whl.metadata\n  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: altair<6.0,>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.41.2->-r requirements_versions.txt (line 13)) (5.2.0)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from gradio==3.41.2->-r requirements_versions.txt (line 13)) (0.101.1)\nCollecting ffmpy (from gradio==3.41.2->-r requirements_versions.txt (line 13))\n  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting gradio-client==0.5.0 (from gradio==3.41.2->-r requirements_versions.txt (line 13))\n  Obtaining dependency information for gradio-client==0.5.0 from https://files.pythonhosted.org/packages/fe/85/ec0323f39192c4bee04e8e06e64213aff816b9d1b61c3c8367e75b1c7e10/gradio_client-0.5.0-py3-none-any.whl.metadata\n  Downloading gradio_client-0.5.0-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio==3.41.2->-r requirements_versions.txt (line 13)) (5.13.0)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.41.2->-r requirements_versions.txt (line 13)) (3.1.2)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.41.2->-r requirements_versions.txt (line 13)) (2.1.3)\nRequirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.41.2->-r requirements_versions.txt (line 13)) (3.7.4)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.41.2->-r requirements_versions.txt (line 13)) (3.9.5)\nRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.41.2->-r requirements_versions.txt (line 13)) (2.0.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from gradio==3.41.2->-r requirements_versions.txt (line 13)) (1.10.12)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio==3.41.2->-r requirements_versions.txt (line 13)) (0.25.1)\nCollecting python-multipart (from gradio==3.41.2->-r requirements_versions.txt (line 13))\n  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting semantic-version~=2.0 (from gradio==3.41.2->-r requirements_versions.txt (line 13))\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nRequirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.41.2->-r requirements_versions.txt (line 13)) (0.23.2)\nCollecting websockets<12.0,>=10.0 (from gradio==3.41.2->-r requirements_versions.txt (line 13))\n  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: cffi>=1.9.1 in /opt/conda/lib/python3.10/site-packages (from pygit2==1.12.2->-r requirements_versions.txt (line 14)) (1.15.1)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx==0.24.1->-r requirements_versions.txt (line 16)) (2023.11.17)\nCollecting httpcore<0.18.0,>=0.15.0 (from httpx==0.24.1->-r requirements_versions.txt (line 16))\n  Obtaining dependency information for httpcore<0.18.0,>=0.15.0 from https://files.pythonhosted.org/packages/94/2c/2bde7ff8dd2064395555220cbf7cba79991172bf5315a07eb3ac7688d9f1/httpcore-0.17.3-py3-none-any.whl.metadata\n  Downloading httpcore-0.17.3-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx==0.24.1->-r requirements_versions.txt (line 16)) (3.4)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx==0.24.1->-r requirements_versions.txt (line 16)) (1.3.0)\nCollecting coloredlogs (from onnxruntime==1.16.3->-r requirements_versions.txt (line 17))\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime==1.16.3->-r requirements_versions.txt (line 17)) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime==1.16.3->-r requirements_versions.txt (line 17)) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime==1.16.3->-r requirements_versions.txt (line 17)) (1.12)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm==0.9.2->-r requirements_versions.txt (line 18)) (0.15.1)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==3.41.2->-r requirements_versions.txt (line 13)) (4.19.0)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==3.41.2->-r requirements_versions.txt (line 13)) (0.12.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.9.1->pygit2==1.12.2->-r requirements_versions.txt (line 14)) (2.21)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning==1.9.4->-r requirements_versions.txt (line 11)) (3.8.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx==0.24.1->-r requirements_versions.txt (line 16)) (0.14.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx==0.24.1->-r requirements_versions.txt (line 16)) (3.7.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.6.0.post0->pytorch_lightning==1.9.4->-r requirements_versions.txt (line 11)) (68.1.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.41.2->-r requirements_versions.txt (line 13)) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.41.2->-r requirements_versions.txt (line 13)) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.41.2->-r requirements_versions.txt (line 13)) (4.42.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.41.2->-r requirements_versions.txt (line 13)) (1.4.4)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.41.2->-r requirements_versions.txt (line 13)) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.41.2->-r requirements_versions.txt (line 13)) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==3.41.2->-r requirements_versions.txt (line 13)) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==3.41.2->-r requirements_versions.txt (line 13)) (2023.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements_versions.txt (line 3)) (3.2.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.2->-r requirements_versions.txt (line 3)) (1.26.15)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->torchsde==0.2.5->-r requirements_versions.txt (line 1)) (3.1)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio==3.41.2->-r requirements_versions.txt (line 13)) (8.1.7)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime==1.16.3->-r requirements_versions.txt (line 17))\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio==3.41.2->-r requirements_versions.txt (line 13)) (0.27.0)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime==1.16.3->-r requirements_versions.txt (line 17)) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4->-r requirements_versions.txt (line 11)) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4->-r requirements_versions.txt (line 11)) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4->-r requirements_versions.txt (line 11)) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4->-r requirements_versions.txt (line 11)) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4->-r requirements_versions.txt (line 11)) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4->-r requirements_versions.txt (line 11)) (1.3.1)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx==0.24.1->-r requirements_versions.txt (line 16)) (1.1.3)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.41.2->-r requirements_versions.txt (line 13)) (2023.7.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.41.2->-r requirements_versions.txt (line 13)) (0.30.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.41.2->-r requirements_versions.txt (line 13)) (0.9.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==3.41.2->-r requirements_versions.txt (line 13)) (1.16.0)\nDownloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gradio-3.41.2-py3-none-any.whl (20.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading pygit2-1.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading opencv_contrib_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading httpx-0.24.1-py3-none-any.whl (75 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gradio_client-0.5.0-py3-none-any.whl (298 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.2/298.2 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\nDownloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime, ffmpy\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=29146f5fe42570e06697396ffd8e6bf6383bd8602fe5cef48559873dab8cc0c5\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=25518e198b0f8bff8e0b9b29ac4664b32f712abc313f88e5bb63bcb796c6c7ab\n  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\nSuccessfully built antlr4-python3-runtime ffmpy\n\u001b[33mDEPRECATION: torchsde 0.2.5 has a non-standard dependency specifier numpy>=1.19.*; python_version >= \"3.7\". pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of torchsde or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: trampoline, tokenizers, safetensors, ffmpy, einops, antlr4-python3-runtime, websockets, tqdm, semantic-version, scipy, pyyaml, python-multipart, psutil, Pillow, opencv-contrib-python, humanfriendly, aiofiles, pygit2, omegaconf, httpcore, coloredlogs, transformers, torchsde, onnxruntime, httpx, accelerate, timm, pytorch_lightning, gradio-client, gradio\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.0\n    Uninstalling tokenizers-0.15.0:\n      Successfully uninstalled tokenizers-0.15.0\n  Attempting uninstall: safetensors\n    Found existing installation: safetensors 0.4.1\n    Uninstalling safetensors-0.4.1:\n      Successfully uninstalled safetensors-0.4.1\n  Attempting uninstall: websockets\n    Found existing installation: websockets 12.0\n    Uninstalling websockets-12.0:\n      Successfully uninstalled websockets-12.0\n  Attempting uninstall: tqdm\n    Found existing installation: tqdm 4.66.1\n    Uninstalling tqdm-4.66.1:\n      Successfully uninstalled tqdm-4.66.1\n  Attempting uninstall: scipy\n    Found existing installation: SciPy 1.11.4\n    Uninstalling SciPy-1.11.4:\n      Successfully uninstalled SciPy-1.11.4\n  Attempting uninstall: pyyaml\n    Found existing installation: PyYAML 6.0.1\n    Uninstalling PyYAML-6.0.1:\n      Successfully uninstalled PyYAML-6.0.1\n  Attempting uninstall: psutil\n    Found existing installation: psutil 5.9.3\n    Uninstalling psutil-5.9.3:\n      Successfully uninstalled psutil-5.9.3\n  Attempting uninstall: Pillow\n    Found existing installation: Pillow 10.1.0\n    Uninstalling Pillow-10.1.0:\n      Successfully uninstalled Pillow-10.1.0\n  Attempting uninstall: opencv-contrib-python\n    Found existing installation: opencv-contrib-python 4.8.1.78\n    Uninstalling opencv-contrib-python-4.8.1.78:\n      Successfully uninstalled opencv-contrib-python-4.8.1.78\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.36.0\n    Uninstalling transformers-4.36.0:\n      Successfully uninstalled transformers-4.36.0\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.25.0\n    Uninstalling accelerate-0.25.0:\n      Successfully uninstalled accelerate-0.25.0\n  Attempting uninstall: timm\n    Found existing installation: timm 0.9.12\n    Uninstalling timm-0.9.12:\n      Successfully uninstalled timm-0.9.12\n  Attempting uninstall: pytorch_lightning\n    Found existing installation: pytorch-lightning 2.1.2\n    Uninstalling pytorch-lightning-2.1.2:\n      Successfully uninstalled pytorch-lightning-2.1.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nbeatrix-jupyterlab 2023.814.150030 requires jupyter-server~=1.16, but you have jupyter-server 2.12.1 which is incompatible.\nbeatrix-jupyterlab 2023.814.150030 requires jupyterlab~=3.4, but you have jupyterlab 4.0.5 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\nfeaturetools 1.28.0 requires scipy>=1.10.0, but you have scipy 1.9.3 which is incompatible.\nfitter 1.6.0 requires tqdm<5.0.0,>=4.65.1, but you have tqdm 4.64.1 which is incompatible.\njupyterlab 4.0.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyterlab<5.0.0a0,>=4.0.6, but you have jupyterlab 4.0.5 which is incompatible.\nkaggle-environments 1.14.3 requires scipy>=1.11.2, but you have scipy 1.9.3 which is incompatible.\nkaggle-environments 1.14.3 requires transformers>=4.33.1, but you have transformers 4.30.2 which is incompatible.\nkfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.9.3 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nwoodwork 0.27.0 requires scipy>=1.10.0, but you have scipy 1.9.3 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed Pillow-9.2.0 accelerate-0.21.0 aiofiles-23.2.1 antlr4-python3-runtime-4.9.3 coloredlogs-15.0.1 einops-0.4.1 ffmpy-0.3.1 gradio-3.41.2 gradio-client-0.5.0 httpcore-0.17.3 httpx-0.24.1 humanfriendly-10.0 omegaconf-2.2.3 onnxruntime-1.16.3 opencv-contrib-python-4.8.0.74 psutil-5.9.5 pygit2-1.12.2 python-multipart-0.0.6 pytorch_lightning-1.9.4 pyyaml-6.0.1 safetensors-0.3.1 scipy-1.7.3 semantic-version-2.10.0 timm-0.9.2 tokenizers-0.13.3 torchsde-0.2.5 tqdm-4.64.1 trampoline-0.1.2 transformers-4.30.2 websockets-11.0.3\nLooking in indexes: https://download.pytorch.org/whl/cu117\nCollecting torch\n  Downloading https://download.pytorch.org/whl/cu117/torch-2.0.1%2Bcu117-cp310-cp310-linux_x86_64.whl (1843.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m459.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchvision\n  Downloading https://download.pytorch.org/whl/cu117/torchvision-0.15.2%2Bcu117-cp310-cp310-linux_x86_64.whl (6.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting filelock (from torch)\n  Downloading https://download.pytorch.org/whl/filelock-3.9.0-py3-none-any.whl (9.7 kB)\nCollecting typing-extensions (from torch)\n  Downloading https://download.pytorch.org/whl/typing_extensions-4.4.0-py3-none-any.whl (26 kB)\nCollecting sympy (from torch)\n  Downloading https://download.pytorch.org/whl/sympy-1.12-py3-none-any.whl (5.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting networkx (from torch)\n  Downloading https://download.pytorch.org/whl/networkx-3.0-py3-none-any.whl (2.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting jinja2 (from torch)\n  Downloading https://download.pytorch.org/whl/Jinja2-3.1.2-py3-none-any.whl (133 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting triton==2.0.0 (from torch)\n  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting cmake (from triton==2.0.0->torch)\n  Downloading https://download.pytorch.org/whl/cmake-3.25.0-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting lit (from triton==2.0.0->torch)\n  Downloading https://download.pytorch.org/whl/lit-15.0.7.tar.gz (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting numpy (from torchvision)\n  Downloading https://download.pytorch.org/whl/numpy-1.24.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting requests (from torchvision)\n  Downloading https://download.pytorch.org/whl/requests-2.28.1-py3-none-any.whl (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n  Downloading https://download.pytorch.org/whl/Pillow-9.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2->torch)\n  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nCollecting charset-normalizer<3,>=2 (from requests->torchvision)\n  Downloading https://download.pytorch.org/whl/charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\nCollecting idna<4,>=2.5 (from requests->torchvision)\n  Downloading https://download.pytorch.org/whl/idna-3.4-py3-none-any.whl (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting urllib3<1.27,>=1.21.1 (from requests->torchvision)\n  Downloading https://download.pytorch.org/whl/urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting certifi>=2017.4.17 (from requests->torchvision)\n  Downloading https://download.pytorch.org/whl/certifi-2022.12.7-py3-none-any.whl (155 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting mpmath>=0.19 (from sympy->torch)\n  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: lit\n  Building wheel for lit (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=89989 sha256=6d13b8ffb0535a2e1a37b8863fefa880dbd8bd4fd74c80f0aad57c581305cbbd\n  Stored in directory: /root/.cache/pip/wheels/27/2c/b6/3ed2983b1b44fe0dea1bb35234b09f2c22fb8ebb308679c922\nSuccessfully built lit\n\u001b[33mWARNING: Error parsing requirements for pyyaml: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/PyYAML-6.0.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mDEPRECATION: torchsde 0.2.5 has a non-standard dependency specifier numpy>=1.19.*; python_version >= \"3.7\". pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of torchsde or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: mpmath, lit, cmake, urllib3, typing-extensions, sympy, pillow, numpy, networkx, MarkupSafe, idna, filelock, charset-normalizer, certifi, requests, jinja2, triton, torch, torchvision\n  Attempting uninstall: mpmath\n    Found existing installation: mpmath 1.3.0\n    Uninstalling mpmath-1.3.0:\n      Successfully uninstalled mpmath-1.3.0\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 1.26.15\n    Uninstalling urllib3-1.26.15:\n      Successfully uninstalled urllib3-1.26.15\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.5.0\n    Uninstalling typing_extensions-4.5.0:\n      Successfully uninstalled typing_extensions-4.5.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.12\n    Uninstalling sympy-1.12:\n      Successfully uninstalled sympy-1.12\n  Attempting uninstall: pillow\n    Found existing installation: Pillow 9.2.0\n    Uninstalling Pillow-9.2.0:\n      Successfully uninstalled Pillow-9.2.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.24.3\n    Uninstalling numpy-1.24.3:\n      Successfully uninstalled numpy-1.24.3\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.1\n    Uninstalling networkx-3.1:\n      Successfully uninstalled networkx-3.1\n  Attempting uninstall: MarkupSafe\n    Found existing installation: MarkupSafe 2.1.3\n    Uninstalling MarkupSafe-2.1.3:\n      Successfully uninstalled MarkupSafe-2.1.3\n  Attempting uninstall: idna\n    Found existing installation: idna 3.4\n    Uninstalling idna-3.4:\n      Successfully uninstalled idna-3.4\n  Attempting uninstall: filelock\n    Found existing installation: filelock 3.12.2\n    Uninstalling filelock-3.12.2:\n      Successfully uninstalled filelock-3.12.2\n  Attempting uninstall: charset-normalizer\n    Found existing installation: charset-normalizer 3.2.0\n    Uninstalling charset-normalizer-3.2.0:\n      Successfully uninstalled charset-normalizer-3.2.0\n  Attempting uninstall: certifi\n    Found existing installation: certifi 2023.11.17\n    Uninstalling certifi-2023.11.17:\n      Successfully uninstalled certifi-2023.11.17\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\n  Attempting uninstall: jinja2\n    Found existing installation: Jinja2 3.1.2\n    Uninstalling Jinja2-3.1.2:\n      Successfully uninstalled Jinja2-3.1.2\n  Attempting uninstall: torch\n    Found existing installation: torch 2.0.0\n    Uninstalling torch-2.0.0:\n      Successfully uninstalled torch-2.0.0\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.15.1\n    Uninstalling torchvision-0.15.1:\n      Successfully uninstalled torchvision-0.15.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naccelerate 0.21.0 requires pyyaml, which is not installed.\nalbumentations 1.3.1 requires PyYAML, which is not installed.\nastropy 6.0.0 requires PyYAML>=3.13, which is not installed.\nbokeh 3.3.2 requires PyYAML>=3.10, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndistributed 2023.12.0 requires pyyaml>=5.3.1, which is not installed.\neasyocr 1.7.1 requires PyYAML, which is not installed.\nessentia 2.1b6.dev1110 requires pyyaml, which is not installed.\nfastai 2.7.13 requires pyyaml, which is not installed.\nflax 0.7.5 requires PyYAML>=5.4.1, which is not installed.\ngradio 3.41.2 requires pyyaml<7.0,>=5.0, which is not installed.\nhuggingface-hub 0.19.4 requires pyyaml>=5.1, which is not installed.\nkfp 2.0.1 requires PyYAML<7,>=5.3, which is not installed.\nkubernetes 26.1.0 requires pyyaml>=5.4.1, which is not installed.\noptuna 3.5.0 requires PyYAML, which is not installed.\norbax-checkpoint 0.4.7 requires pyyaml, which is not installed.\npapermill 2.4.0 requires pyyaml, which is not installed.\npytorch-lightning 1.9.4 requires PyYAML>=5.4, which is not installed.\nray 2.6.3 requires pyyaml, which is not installed.\ntimm 0.9.2 requires pyyaml, which is not installed.\ntransformers 4.30.2 requires pyyaml>=5.1, which is not installed.\nvaex-core 4.17.1 requires pyyaml, which is not installed.\nwandb 0.16.1 requires PyYAML, which is not installed.\nydata-profiling 4.5.1 requires PyYAML<6.1,>=5.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nbeatrix-jupyterlab 2023.814.150030 requires jupyter-server~=1.16, but you have jupyter-server 2.12.1 which is incompatible.\nbeatrix-jupyterlab 2023.814.150030 requires jupyterlab~=3.4, but you have jupyterlab 4.0.5 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ncuml 23.8.0 requires scipy>=1.8.0, but you have scipy 1.7.3 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\nfastapi 0.101.1 requires typing-extensions>=4.5.0, but you have typing-extensions 4.4.0 which is incompatible.\nfeaturetools 1.28.0 requires scipy>=1.10.0, but you have scipy 1.7.3 which is incompatible.\nfitter 1.6.0 requires tqdm<5.0.0,>=4.65.1, but you have tqdm 4.64.1 which is incompatible.\ngcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.12.2 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-pubsub 2.18.3 requires grpcio<2.0dev,>=1.51.3, but you have grpcio 1.51.1 which is incompatible.\ninequality 1.0.1 requires scipy>=1.8, but you have scipy 1.7.3 which is incompatible.\njax 0.4.21 requires scipy>=1.9, but you have scipy 1.7.3 which is incompatible.\njaxlib 0.4.21+cuda11.cudnn86 requires scipy>=1.9, but you have scipy 1.7.3 which is incompatible.\njupyterlab 4.0.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyterlab<5.0.0a0,>=4.0.6, but you have jupyterlab 4.0.5 which is incompatible.\nkaggle-environments 1.14.3 requires scipy>=1.11.2, but you have scipy 1.7.3 which is incompatible.\nkaggle-environments 1.14.3 requires transformers>=4.33.1, but you have transformers 4.30.2 which is incompatible.\nkfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires scipy>=1.8, but you have scipy 1.7.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.4.0 requires scipy>=1.8, but you have scipy 1.7.3 which is incompatible.\npydantic-core 2.14.5 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.4.0 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\nscikit-image 0.21.0 requires scipy>=1.8, but you have scipy 1.7.3 which is incompatible.\nscipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.24.1 which is incompatible.\nspglm 1.1.0 requires scipy>=1.8, but you have scipy 1.7.3 which is incompatible.\nspopt 0.6.0 requires scipy>=1.8, but you have scipy 1.7.3 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nvirtualenv 20.21.0 requires platformdirs<4,>=2.4, but you have platformdirs 4.1.0 which is incompatible.\nwoodwork 0.27.0 requires scipy>=1.10.0, but you have scipy 1.7.3 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed MarkupSafe-2.1.3 certifi-2022.12.7 charset-normalizer-2.1.1 cmake-3.25.0 filelock-3.9.0 idna-3.4 jinja2-3.1.2 lit-15.0.7 mpmath-1.3.0 networkx-3.0 numpy-1.24.1 pillow-9.3.0 requests-2.28.1 sympy-1.12 torch-2.0.1+cu117 torchvision-0.15.2+cu117 triton-2.0.0 typing-extensions-4.4.0 urllib3-1.26.13\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (6.0)\nCollecting pytz\n  Obtaining dependency information for pytz from https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl.metadata\n  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\nDownloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.5/502.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[33mDEPRECATION: torchsde 0.2.5 has a non-standard dependency specifier numpy>=1.19.*; python_version >= \"3.7\". pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of torchsde or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: pytz\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\npointpats 2.4.0 requires scipy>=1.8, but you have scipy 1.7.3 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.1 which is incompatible.\nspopt 0.6.0 requires scipy>=1.8, but you have scipy 1.7.3 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nwoodwork 0.27.0 requires scipy>=1.10.0, but you have scipy 1.7.3 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pytz-2023.3.post1\n\nLooking for: ['openssh']\n\n\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/c6f2354e.json\" was modified by another program\n\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\nrapidsai/linux-64 (check zst) \u001b[33m━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━\u001b[0m   0.0 B @  ??.?MB/s Checking  0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[0Grapidsai/linux-64 (check zst)                       Checked  0.2s\n\u001b[?25h\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/86b0f08d.json\" was modified by another program\n\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\nrapidsai/noarch (check zst) \u001b[90m━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s Checking  0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[0Grapidsai/noarch (check zst)                         Checked  0.1s\n\u001b[?25h\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/c9ddbd6b.json\" was modified by another program\n\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.1s\nnvidia/linux-64 (check zst) \u001b[90m━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s Checking  0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[0Gnvidia/linux-64 (check zst)                         Checked  0.1s\n\u001b[?25h\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/b121c3e7.json\" was modified by another program\n\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\nnvidia/noarch (check zst) \u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━\u001b[0m   0.0 B @  ??.?MB/s Checking  0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[0Gnvidia/noarch (check zst)                           Checked  0.1s\n\u001b[?25h\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/497deca9.json\" was modified by another program\n\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/09cdf8bf.json\" was modified by another program\n\u001b[?25l\u001b[2K\u001b[0G\u001b[?25h\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/47929eba.json\" was modified by another program\n\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/main/linux-64 (check zst)                     Checked  0.1s\n\u001b[?25h\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/3e39a7aa.json\" was modified by another program\n\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\npkgs/main/noarch (check zst) \u001b[33m━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s Checking  0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/main/noarch (check zst)                        Checked  0.0s\n\u001b[?25h\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/2ce54b42.json\" was modified by another program\n\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\n\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/r/linux-64 (check zst)                        Checked  0.0s\n\u001b[?25h\u001b[33m\u001b[1mwarning  libmamba\u001b[m Cache file \"/opt/conda/pkgs/cache/4ea078d6.json\" was modified by another program\n\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\npkgs/r/noarch (check zst) \u001b[90m━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━\u001b[0m   0.0 B @  ??.?MB/s Checking  0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/r/noarch (check zst)                           Checked  0.0s\n\u001b[?25h\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\nrapidsai/linux-64 \u001b[90m━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\nrapidsai/noarch   \u001b[90m━━━━━━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gnvidia/noarch                                        4.9kB @  66.4kB/s  0.1s\n[+] 0.1s\nconda-forge/linux-64 \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m  29.4kB /  31.6MB @ 419.9kB/s  0.1s\nrapidsai/linux-64    \u001b[90m━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\nrapidsai/noarch      \u001b[33m━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\nnvidia/linux-64      \u001b[90m━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\npkgs/r/noarch        \u001b[90m━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Grapidsai/noarch                                      6.2kB @  51.3kB/s  0.1s\nnvidia/linux-64                                    159.0kB @   1.3MB/s  0.1s\n[+] 0.2s\nconda-forge/linux-64 ━━━╸\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m   6.0MB /  31.6MB @  31.4MB/s  0.2s\nconda-forge/noarch   \u001b[90m━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\nrapidsai/linux-64    ━━━━━━━━━━━━━━━━╸\u001b[90m━━━━━━\u001b[0m 194.9kB / 262.2kB @   1.1MB/s  0.2s\npkgs/r/linux-64      \u001b[33m━━━━━━━╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\npkgs/r/noarch        \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m  85.6kB /   2.0MB @ 592.2kB/s  0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Grapidsai/linux-64                                  262.2kB @   1.2MB/s  0.3s\n[+] 0.3s\nconda-forge/linux-64 ━━━━━╸\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m   9.3MB /  31.6MB @  31.8MB/s  0.3s\nconda-forge/noarch   ━━━━╸\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m   2.9MB /  13.1MB @  11.1MB/s  0.2s\npkgs/main/noarch     \u001b[90m━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\npkgs/r/linux-64      \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m  77.1kB /   1.6MB @ 317.4kB/s  0.2s\npkgs/r/noarch        ━━━━╸\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m 506.8kB /   2.0MB @   2.1MB/s  0.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/r/noarch                                        2.0MB @   5.8MB/s  0.3s\n[+] 0.4s\nconda-forge/linux-64 ━━━━━━╸\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m  10.9MB /  31.6MB @  31.7MB/s  0.4s\nconda-forge/noarch   ━━━━━━━━━╸\u001b[90m━━━━━━━━━━━━━\u001b[0m   6.2MB /  13.1MB @  16.7MB/s  0.3s\npkgs/main/linux-64   \u001b[90m━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.0s\npkgs/main/noarch     \u001b[90m━━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━\u001b[0m   0.0 B /  ??.?MB @  ??.?MB/s  0.1s\npkgs/r/linux-64      ━━━╸\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m 326.6kB /   1.6MB @ 918.1kB/s  0.3s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.5s\nconda-forge/linux-64 ━━━━━━━━━╸\u001b[90m━━━━━━━━━━━━━\u001b[0m  14.2MB /  31.6MB @  29.9MB/s  0.5s\nconda-forge/noarch   ━━━━━━━━━━━━╸\u001b[90m━━━━━━━━━━\u001b[0m   7.9MB /  13.1MB @  18.1MB/s  0.4s\npkgs/main/linux-64   ━━━╸\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m   1.1MB /   5.7MB @   2.3MB/s  0.1s\npkgs/main/noarch     ━╸\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m  85.6kB / 698.6kB @ 195.2kB/s  0.2s\npkgs/r/linux-64      ━━━━━━━━━━━━━━━━━━╸\u001b[90m━━━━\u001b[0m   1.4MB /   1.6MB @   2.8MB/s  0.4s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 0.6s\nconda-forge/linux-64 ━━━━━━━━━━╸\u001b[90m━━━━━━━━━━━━\u001b[0m  15.9MB /  31.6MB @  29.2MB/s  0.6s\nconda-forge/noarch   ━━━━━━━━━━━━━━━━━━╸\u001b[90m━━━━\u001b[0m  11.2MB /  13.1MB @  19.7MB/s  0.5s\npkgs/main/linux-64   ━━━━━━━━━━━━━━━━╸\u001b[90m━━━━━━\u001b[0m   4.4MB /   5.7MB @   7.4MB/s  0.2s\npkgs/main/noarch     ━━━━━━━━━━━━╸\u001b[90m━━━━━━━━━━\u001b[0m 424.9kB / 698.6kB @ 744.6kB/s  0.3s\npkgs/r/linux-64      ━━━━━━━━━━━━━━━━━━━━━━━   1.6MB /   1.6MB @   2.9MB/s  0.5s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/r/linux-64                                      1.6MB @   2.9MB/s  0.5s\npkgs/main/noarch                                   698.6kB @   1.1MB/s  0.4s\n[+] 0.7s\nconda-forge/linux-64 ━━━━━━━━━━━╸\u001b[90m━━━━━━━━━━━\u001b[0m  17.5MB /  31.6MB @  28.0MB/s  0.7s\nconda-forge/noarch   ━━━━━━━━━━━━━━━━━━━━━╸\u001b[90m━\u001b[0m  12.8MB /  13.1MB @  19.9MB/s  0.6s\npkgs/main/linux-64   ━━━━━━━━━━━━━━━━━━━━━━━   5.7MB /   5.7MB @   8.6MB/s  0.3s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gpkgs/main/linux-64                                   5.7MB @   8.6MB/s  0.3s\n[+] 0.8s\nconda-forge/linux-64 ━━━━━━━━━━━━╸\u001b[90m━━━━━━━━━━\u001b[0m  19.2MB /  31.6MB @  26.1MB/s  0.8s\nconda-forge/noarch   ━━━━━━━━━━━━━━━━━━━━━━━  13.1MB /  13.1MB @  17.7MB/s  0.7s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/noarch                                  13.1MB @  17.7MB/s  0.7s\n[+] 0.9s\nconda-forge/linux-64 ━━━━━━━━━━━━━━━━━╸\u001b[90m━━━━━\u001b[0m  25.8MB /  31.6MB @  28.9MB/s  0.9s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.0s\nconda-forge/linux-64 ━━━━━━━━━━━━━━━━━━━━━╸\u001b[90m━\u001b[0m  30.7MB /  31.6MB @  32.1MB/s  1.0s\u001b[2K\u001b[1A\u001b[2K\u001b[0G[+] 1.1s\nconda-forge/linux-64 ━━━━━━━━━━━━━━━━━━━━━╸\u001b[90m━\u001b[0m  30.7MB /  31.6MB @  32.1MB/s  1.1s\u001b[2K\u001b[1A\u001b[2K\u001b[0Gconda-forge/linux-64                                31.6MB @  32.7MB/s  1.2s\n\u001b[?25h\nPinned packages:\n  - python 3.10.*\n\n\nTransaction\n\n  Prefix: /opt/conda\n\n  Updating specs:\n\n   - openssh\n   - ca-certificates\n   - certifi\n   - openssl\n\n\n  Package      Version  Build       Channel          Size\n───────────────────────────────────────────────────────────\n  Install:\n───────────────────────────────────────────────────────────\n\n  \u001b[32m+ libcbor  \u001b[0m   0.10.2  hcb278e6_0  conda-forge      43kB\n  \u001b[32m+ libxcrypt\u001b[0m   4.4.36  hd590300_1  conda-forge     100kB\n  \u001b[32m+ attr     \u001b[0m    2.5.1  h166bdaf_1  conda-forge      71kB\n  \u001b[32m+ libcap   \u001b[0m     2.69  h0f662aa_0  conda-forge     101kB\n  \u001b[32m+ libudev1 \u001b[0m      255  h3f72095_0  conda-forge     125kB\n  \u001b[32m+ libfido2 \u001b[0m   1.14.0  h4446dcb_0  conda-forge     272kB\n  \u001b[32m+ openssh  \u001b[0m    9.6p1  h2d3b35a_0  conda-forge     989kB\n\n  Summary:\n\n  Install: 7 packages\n\n  Total download: 2MB\n\n───────────────────────────────────────────────────────────\n\n\n\u001b[?25l\u001b[2K\u001b[0G[+] 0.0s\nDownloading      \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m   0.0 B                            0.0s\nExtracting       \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m       0                            0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Glibcbor                                             42.8kB @ 432.6kB/s  0.1s\n[+] 0.1s\nDownloading  (5) \u001b[33m━━━━━━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━\u001b[0m  42.8kB attr                       0.0s\nExtracting   (1) \u001b[90m━━━━━━━━━━━╸\u001b[0m\u001b[33m━━━━━━━━━━━\u001b[0m       0 libcbor                    0.0s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Glibxcrypt                                          100.4kB @ 798.8kB/s  0.1s\nattr                                                71.0kB @ 563.5kB/s  0.1s\nlibcap                                             100.6kB @ 563.5kB/s  0.2s\nlibudev1                                           125.2kB @ 700.2kB/s  0.2s\n[+] 0.2s\nDownloading  (2) ━━━━━━━━━━━╸\u001b[33m━━━━━━━━━━━\u001b[0m 956.3kB libfido2                   0.1s\nExtracting   (4) ━━╸\u001b[33m━━━━━━━━━━━━━╸\u001b[0m\u001b[90m━━━━━━\u001b[0m       1 libcbor                    0.1s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0Glibfido2                                           271.5kB @   1.1MB/s  0.1s\nopenssh                                            988.8kB @   3.6MB/s  0.2s\n[+] 0.3s\nDownloading      ━━━━━━━━━━━━━━━━━━━━━━━   1.7MB                            0.2s\nExtracting   (3) ━━━━━━━━━━━━╸\u001b[33m━━━━━━━━━━\u001b[0m       4 libfido2                   0.2s\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[0G\u001b[?25h\nDownloading and Extracting Packages\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n","output_type":"stream"}]},{"cell_type":"code","source":"# Starting the Web UI with ngrok\n\n# --- Variables ---\n\nNgrok_token = \"2aU1Arvj9Uj4x3ATr39OQEsvZUm_86mXwH2fpniqKdVkYL1Q3\"  # Update with your ngrok token\nNgrok_domain = \"\"  # optional, leave empty if you don't have a domain\nport = 7865\n\n# -----------------\n\n!pip install pyngrok==6.1.0  # Downgrade to a version that supports ngrok tunneling\n\nfrom pyngrok import ngrok, conf\nimport gc\n\ngc.collect()\n\nif Ngrok_token:\n    try:\n        ngrok.set_auth_token(Ngrok_token)\n        ngrok.kill()\n        srv = ngrok.connect(port)\n        print(f\"Ngrok Tunnel is active at: {srv.public_url}\")\n        get_ipython().system(f\"python {HOME_FOLDER}/Fooocus/entry_with_update.py\")\n    except Exception as e:\n        print(f\"Error starting ngrok tunnel: {e}\")\nelse:\n    print('An ngrok token is required. You can get one on https://ngrok.com and paste it into the Ngrok_token field.')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T10:50:04.248137Z","iopub.execute_input":"2024-01-04T10:50:04.248559Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting pyngrok==6.1.0\n  Downloading pyngrok-6.1.0.tar.gz (698 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.7/698.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from pyngrok==6.1.0) (6.0)\nBuilding wheels for collected packages: pyngrok\n  Building wheel for pyngrok (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyngrok: filename=pyngrok-6.1.0-py3-none-any.whl size=20583 sha256=62a74793376fb0e3e5dbae6f59d79c2eb7d49ec0b9f62874273dbfd987d9affe\n  Stored in directory: /root/.cache/pip/wheels/d8/2d/7a/97a039fca211fa789bffad50ff97dca13c01e9b83e8879f503\nSuccessfully built pyngrok\n\u001b[33mDEPRECATION: torchsde 0.2.5 has a non-standard dependency specifier numpy>=1.19.*; python_version >= \"3.7\". pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of torchsde or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: pyngrok\nSuccessfully installed pyngrok-6.1.0\nNgrok Tunnel is active at: https://b0f1-34-171-12-202.ngrok-free.app                                \nAlready up-to-date\nUpdate succeeded.\n[System ARGV] ['/kaggle/working/Fooocus/entry_with_update.py']\nPython 3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:40:32) [GCC 12.3.0]\nFooocus version: 2.1.860\nInstalling requirements\nDownloading: \"https://huggingface.co/lllyasviel/fav_models/resolve/main/fav/juggernautXL_version6Rundiffusion.safetensors\" to /kaggle/working/Fooocus/models/checkpoints/juggernautXL_version6Rundiffusion.safetensors\n\n100%|███████████████████████████████████████| 6.62G/6.62G [00:28<00:00, 254MB/s]\nDownloading: \"https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_offset_example-lora_1.0.safetensors\" to /kaggle/working/Fooocus/models/loras/sd_xl_offset_example-lora_1.0.safetensors\n\n100%|███████████████████████████████████████| 47.3M/47.3M [00:00<00:00, 197MB/s]\nDownloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/xlvaeapp.pth\" to /kaggle/working/Fooocus/models/vae_approx/xlvaeapp.pth\n\n100%|████████████████████████████████████████| 209k/209k [00:00<00:00, 6.08MB/s]\nDownloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/vaeapp_sd15.pt\" to /kaggle/working/Fooocus/models/vae_approx/vaeapp_sd15.pth\n\n100%|████████████████████████████████████████| 209k/209k [00:00<00:00, 6.14MB/s]\nDownloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/xl-to-v1_interposer-v3.1.safetensors\" to /kaggle/working/Fooocus/models/vae_approx/xl-to-v1_interposer-v3.1.safetensors\n\n100%|██████████████████████████████████████| 6.25M/6.25M [00:00<00:00, 71.0MB/s]\nDownloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/fooocus_expansion.bin\" to /kaggle/working/Fooocus/models/prompt_expansion/fooocus_expansion/pytorch_model.bin\n\n100%|█████████████████████████████████████████| 335M/335M [00:01<00:00, 244MB/s]\nTotal VRAM 15110 MB, total RAM 32110 MB\nSet vram state to: NORMAL_VRAM\nAlways offload VRAM\nDevice: cuda:0 Tesla T4 : native\nVAE dtype: torch.float32\nUsing pytorch cross attention\nRunning on local URL:  http://127.0.0.1:7865\n\nTo create a public link, set `share=True` in `launch()`.\nRefiner unloaded.\nmodel_type EPS\nUNet ADM Dimension 2816\nUsing pytorch attention in VAE\nWorking with z of shape (1, 4, 32, 32) = 4096 dimensions.\nUsing pytorch attention in VAE\nextra {'cond_stage_model.clip_l.logit_scale', 'cond_stage_model.clip_g.transformer.text_model.embeddings.position_ids', 'cond_stage_model.clip_l.text_projection'}\nBase model loaded: /kaggle/working/Fooocus/models/checkpoints/juggernautXL_version6Rundiffusion.safetensors\nRequest to load LoRAs [['sd_xl_offset_example-lora_1.0.safetensors', 0.1], ['None', 1.0], ['None', 1.0], ['None', 1.0], ['None', 1.0]] for model [/kaggle/working/Fooocus/models/checkpoints/juggernautXL_version6Rundiffusion.safetensors].\nLoaded LoRA [/kaggle/working/Fooocus/models/loras/sd_xl_offset_example-lora_1.0.safetensors] for UNet [/kaggle/working/Fooocus/models/checkpoints/juggernautXL_version6Rundiffusion.safetensors] with 788 keys at weight 0.1.\nFooocus V2 Expansion: Vocab with 642 words.\nFooocus Expansion engine loaded for cuda:0, use_fp16 = True.\nRequested to load SDXLClipModel\nRequested to load GPT2LMHeadModel\nLoading 2 new models\n[Fooocus Model Management] Moving model(s) has taken 7.51 seconds\nApp started successful. Use the app with http://127.0.0.1:7865/ or 127.0.0.1:7865\n[Parameters] Adaptive CFG = 7\n[Parameters] Sharpness = 2\n[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n[Parameters] CFG = 4.0\n[Parameters] Seed = 2665662084053993154\n[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n[Parameters] Steps = 30 - 15\n[Fooocus] Initializing ...\n[Fooocus] Loading models ...\nRefiner unloaded.\n[Fooocus] Processing prompts ...\n[Fooocus] Preparing Fooocus text #1 ...\n[Prompt Expansion] 1 girl, cute, intricate, highly detailed, dynamic background, joyful, delicate, dramatic bright colors, majestic, open atmosphere, tender, warm, shiny, sharp focus, extremely elegant, divine holy light, artistic, mystical, rich deep color, iconic, fine detail, epic, ambient, full composition, radiant, very inspirational, beautiful, illuminated, gorgeous, amazing\n[Fooocus] Preparing Fooocus text #2 ...\n[Prompt Expansion] 1 girl, full color, cool colors, confident, glowing, crisp, extremely detailed, inspired, clear, artistic, cinematic, new creative, positive, loving, relaxed, beautiful, unique, attractive, cute, brave, friendly, calm, pretty, determined, passionate, lucid, touching, vibrant, intricate, lovely, highly coherent, expressive, symmetry, innocent, iconic\n[Fooocus] Encoding positive #1 ...\n[Fooocus Model Management] Moving model(s) has taken 0.13 seconds\n[Fooocus] Encoding positive #2 ...\n[Fooocus] Encoding negative #1 ...\n[Fooocus] Encoding negative #2 ...\n[Parameters] Denoising Strength = 1.0\n[Parameters] Initial Latent shape: Image Space (896, 1152)\nPreparation time: 6.24 seconds\n[Sampler] refiner_swap_method = joint\n[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\nRequested to load SDXL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 3.14 seconds\n100%|███████████████████████████████████████████| 30/30 [00:26<00:00,  1.13it/s]\nRequested to load AutoencoderKL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 0.33 seconds\nImage generated with private log at: /kaggle/working/Fooocus/outputs/2024-01-04/log.html\nGenerating and saving time: 32.92 seconds\n[Sampler] refiner_swap_method = joint\n[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\nRequested to load SDXL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 1.97 seconds\n100%|███████████████████████████████████████████| 30/30 [00:29<00:00,  1.03it/s]\nRequested to load AutoencoderKL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 0.30 seconds\nImage generated with private log at: /kaggle/working/Fooocus/outputs/2024-01-04/log.html\nGenerating and saving time: 34.31 seconds\nRequested to load SDXLClipModel\nRequested to load GPT2LMHeadModel\nLoading 2 new models\n[Fooocus Model Management] Moving model(s) has taken 0.82 seconds\nTotal time: 79.12 seconds\n[Parameters] Adaptive CFG = 7\n[Parameters] Sharpness = 2\n[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n[Parameters] CFG = 4.0\n[Parameters] Seed = 5331001227460143812\n[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n[Parameters] Steps = 30 - 15\n[Fooocus] Initializing ...\n[Fooocus] Loading models ...\nRefiner unloaded.\n[Fooocus] Processing prompts ...\n[Fooocus] Preparing Fooocus text #1 ...\n[Prompt Expansion] 1 nude girl, intricate, highly detailed, elegant, sacred light, sharp focus, romantic, stylish, passionate, color, cinematic, generous, thought, best, dramatic, thoughtful, enchanted, inspired, famous, beautiful, unique, cool, great, artistic, fabulous, epic, fine, amazing detail, enhanced, very nice, illuminated, magnificent, awesome, perfect\n[Fooocus] Preparing Fooocus text #2 ...\n[Prompt Expansion] 1 nude girl, highly intricate detailed, light, sharp focus, elegant, designed, perfect background, vivid colors, ambient dynamic dramatic cinematic atmosphere, radiant wonderful composition, fine detail, full, epic, beautiful, stunning, creative, positive, unique, attractive, delicate, artistic, focused, passionate, amazing, confident, colorful, brilliant, best, pure, awesome, surreal\n[Fooocus] Encoding positive #1 ...\n[Fooocus Model Management] Moving model(s) has taken 0.13 seconds\n[Fooocus] Encoding positive #2 ...\n[Fooocus] Encoding negative #1 ...\n[Fooocus] Encoding negative #2 ...\n[Parameters] Denoising Strength = 1.0\n[Parameters] Initial Latent shape: Image Space (896, 1152)\nPreparation time: 2.45 seconds\n[Sampler] refiner_swap_method = joint\n[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\nRequested to load SDXL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 3.02 seconds\n100%|███████████████████████████████████████████| 30/30 [00:28<00:00,  1.04it/s]\nRequested to load AutoencoderKL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 0.30 seconds\nImage generated with private log at: /kaggle/working/Fooocus/outputs/2024-01-04/log.html\nGenerating and saving time: 35.18 seconds\n[Sampler] refiner_swap_method = joint\n[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\nRequested to load SDXL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 1.72 seconds\n100%|███████████████████████████████████████████| 30/30 [00:29<00:00,  1.02it/s]\nRequested to load AutoencoderKL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 0.30 seconds\nImage generated with private log at: /kaggle/working/Fooocus/outputs/2024-01-04/log.html\nGenerating and saving time: 34.39 seconds\nRequested to load SDXLClipModel\nRequested to load GPT2LMHeadModel\nLoading 2 new models\n[Fooocus Model Management] Moving model(s) has taken 0.77 seconds\nTotal time: 77.96 seconds\n[Parameters] Adaptive CFG = 7\n[Parameters] Sharpness = 2\n[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n[Parameters] CFG = 4.0\n[Parameters] Seed = 5448819776120668064\n[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n[Parameters] Steps = 30 - 15\n[Fooocus] Initializing ...\n[Fooocus] Loading models ...\nRefiner unloaded.\n[Fooocus] Processing prompts ...\n[Fooocus] Preparing Fooocus text #1 ...\n[Prompt Expansion] 1 nude girl perfct butt full body, proportional, scenic, highly detailed, warm light, sharp focus, romantic, sincere, pretty, clear, artistic, deep, cute, innocent, attractive, depicted, epic background, rich colors, intricate, beautiful, elegant, breathtaking, very inspirational, thought, elite, fine, best, dramatic, modern, futuristic, contemporary\n[Fooocus] Preparing Fooocus text #2 ...\n[Prompt Expansion] 1 nude girl perfct butt full body, proportional, attractive, highly detailed, dramatic light, sharp focus, excellent composition, atmosphere, dynamic background, scenic, artistic, great fine detail, cinematic, beautiful, stunning, creative, positive, cute, cheerful, engaging, inspired, color rich, intricate, inspiring, inspirational, vibrant, healthy, pure, coherent, passionate\n[Fooocus] Encoding positive #1 ...\n[Fooocus Model Management] Moving model(s) has taken 0.13 seconds\n[Fooocus] Encoding positive #2 ...\n[Fooocus] Encoding negative #1 ...\n[Fooocus] Encoding negative #2 ...\n[Parameters] Denoising Strength = 1.0\n[Parameters] Initial Latent shape: Image Space (896, 1152)\nPreparation time: 2.42 seconds\n[Sampler] refiner_swap_method = joint\n[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\nRequested to load SDXL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 2.34 seconds\n100%|███████████████████████████████████████████| 30/30 [00:30<00:00,  1.01s/it]\nRequested to load AutoencoderKL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 0.29 seconds\nImage generated with private log at: /kaggle/working/Fooocus/outputs/2024-01-04/log.html\nGenerating and saving time: 36.07 seconds\n[Sampler] refiner_swap_method = joint\n[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\nRequested to load SDXL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 1.74 seconds\n100%|███████████████████████████████████████████| 30/30 [00:30<00:00,  1.02s/it]\nRequested to load AutoencoderKL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 0.30 seconds\nImage generated with private log at: /kaggle/working/Fooocus/outputs/2024-01-04/log.html\nGenerating and saving time: 35.61 seconds\nRequested to load SDXLClipModel\nRequested to load GPT2LMHeadModel\nLoading 2 new models\n[Fooocus Model Management] Moving model(s) has taken 0.79 seconds\nTotal time: 80.03 seconds\n[Parameters] Adaptive CFG = 7\n[Parameters] Sharpness = 2\n[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n[Parameters] CFG = 4.0\n[Parameters] Seed = 3375010938589027249\n[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n[Parameters] Steps = 30 - 15\n[Fooocus] Initializing ...\n[Fooocus] Loading models ...\nRefiner unloaded.\n[Fooocus] Processing prompts ...\n[Fooocus] Preparing Fooocus text #1 ...\n[Prompt Expansion] 1 nude girl perfct butt bending full body, modern fashionable, beautiful, detailed background, highly intricate, elegant, light, clear, color, sharp, focus, adventurous, thought, iconic, epic, cinematic, fine composition, colorful, romantic, vibrant, enhanced, symmetry, magic, aesthetic, pure, illuminated, extremely, focused, amazing detail, professional, artistic\n[Fooocus] Preparing Fooocus text #2 ...\n[Prompt Expansion] 1 nude girl perfct butt bending full body, highly intricate detailed, elegant, delicate, sharp focus, graceful, vibrant, beautiful, sublime, light, crisp, clear, artistic, color, epic, fine detail, cinematic, background, illuminated, professional, quiet, unique, romantic, inspiring, cute, best, gorgeous, marvelous, amazing, wonderful, colorful, intriguing\n[Fooocus] Encoding positive #1 ...\n[Fooocus Model Management] Moving model(s) has taken 0.13 seconds\n[Fooocus] Encoding positive #2 ...\n[Fooocus] Encoding negative #1 ...\n[Fooocus] Encoding negative #2 ...\n[Parameters] Denoising Strength = 1.0\n[Parameters] Initial Latent shape: Image Space (896, 1152)\nPreparation time: 2.37 seconds\n[Sampler] refiner_swap_method = joint\n[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\nRequested to load SDXL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 2.40 seconds\n100%|███████████████████████████████████████████| 30/30 [00:27<00:00,  1.10it/s]\nRequested to load AutoencoderKL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 0.29 seconds\nImage generated with private log at: /kaggle/working/Fooocus/outputs/2024-01-04/log.html\nGenerating and saving time: 33.02 seconds\n[Sampler] refiner_swap_method = joint\n[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\nRequested to load SDXL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 1.75 seconds\n100%|███████████████████████████████████████████| 30/30 [00:30<00:00,  1.02s/it]\nRequested to load AutoencoderKL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 0.30 seconds\nImage generated with private log at: /kaggle/working/Fooocus/outputs/2024-01-04/log.html\nGenerating and saving time: 35.71 seconds\nRequested to load SDXLClipModel\nRequested to load GPT2LMHeadModel\nLoading 2 new models\n[Fooocus Model Management] Moving model(s) has taken 0.78 seconds\nTotal time: 76.79 seconds\nDownloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/model_base_caption_capfilt_large.pth\" to /kaggle/working/Fooocus/models/clip_vision/model_base_caption_capfilt_large.pth\n\n100%|█████████████████████████████████████████| 855M/855M [00:03<00:00, 267MB/s]\nload checkpoint from /kaggle/working/Fooocus/models/clip_vision/model_base_caption_capfilt_large.pth\nRequested to load BLIP_Decoder\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 1.56 seconds\n[Parameters] Adaptive CFG = 7\n[Parameters] Sharpness = 2\n[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n[Parameters] CFG = 4.0\n[Parameters] Seed = 7191911749350062440\n[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n[Parameters] Steps = 30 - 15\n[Fooocus] Initializing ...\n[Fooocus] Loading models ...\nRefiner unloaded.\nRequested to load SDXLClipModel\nRequested to load GPT2LMHeadModel\nLoading 2 new models\n[Fooocus Model Management] Moving model(s) has taken 0.73 seconds\n[Fooocus] Processing prompts ...\n[Fooocus] Preparing Fooocus text #1 ...\n[Prompt Expansion] two red head woman kissing each other on the beach, sharp focus, detailed, elegant, highly colorful, vivid color, intricate, magical atmosphere, iconic, fine, sublime, elaborate, amazing composition, cinematic, complex, artistic, surreal, extremely inspirational, thoughtful, wonderful, thought, professional, inspiring, beautiful, marvelous, joyful, epic, stunning, gorgeous, creative\n[Fooocus] Preparing Fooocus text #2 ...\n[Prompt Expansion] two red head woman kissing each other on the beach, full perfect, intricate, elegant, highly detailed, wonderful colors, glowing, sharp focus, beautiful light, vibrant composition, cute, decorative background, symmetry, iconic, fine detail, clear, pretty, inspired, professional, awarded, extremely, attractive, delicate, artistic, smart, caring, futuristic, famous, gorgeous\n[Fooocus] Encoding positive #1 ...\n[Fooocus Model Management] Moving model(s) has taken 0.12 seconds\n[Fooocus] Encoding positive #2 ...\n[Fooocus] Encoding negative #1 ...\n[Fooocus] Encoding negative #2 ...\n[Parameters] Denoising Strength = 1.0\n[Parameters] Initial Latent shape: Image Space (896, 1152)\nPreparation time: 3.07 seconds\n[Sampler] refiner_swap_method = joint\n[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\nRequested to load SDXL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 2.34 seconds\n100%|███████████████████████████████████████████| 30/30 [00:27<00:00,  1.08it/s]\nRequested to load AutoencoderKL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 0.30 seconds\nImage generated with private log at: /kaggle/working/Fooocus/outputs/2024-01-04/log.html\nGenerating and saving time: 33.61 seconds\n[Sampler] refiner_swap_method = joint\n[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\nRequested to load SDXL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 1.79 seconds\n100%|███████████████████████████████████████████| 30/30 [00:27<00:00,  1.08it/s]\nRequested to load AutoencoderKL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 0.30 seconds\nImage generated with private log at: /kaggle/working/Fooocus/outputs/2024-01-04/log.html\nGenerating and saving time: 32.89 seconds\nRequested to load SDXLClipModel\nRequested to load GPT2LMHeadModel\nLoading 2 new models\n[Fooocus Model Management] Moving model(s) has taken 0.78 seconds\nTotal time: 75.44 seconds\n[Parameters] Adaptive CFG = 7\n[Parameters] Sharpness = 2\n[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n[Parameters] CFG = 4.0\n[Parameters] Seed = 4329534696821726846\n[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n[Parameters] Steps = 30 - 15\n[Fooocus] Initializing ...\n[Fooocus] Loading models ...\nRefiner unloaded.\n[Fooocus] Processing prompts ...\n[Fooocus] Preparing Fooocus text #1 ...\n[Prompt Expansion] two red head woman deep kissing each other on the beach, full body, intricate, elegant, highly detailed, cute, sublime, sharp focus, dramatic light, attractive, professional, smart, charming, best, modern, futuristic, stylish, new, cool, inspired, pretty, cinematic, trendy, amazing, confident, colorful, surreal, beautiful, aesthetic, inspirational, lovely\n[Fooocus] Preparing Fooocus text #2 ...\n[Prompt Expansion] two red head woman deep kissing each other on the beach, full body, intricate, elegant, highly detailed, extremely lush colorful, holy sacred, flowing, light, sharp focus, cinematic, fine composition, beautiful dynamic dramatic bright, surreal, majestic, open great original, inspiring, cute, lovely, epic, professional, artistic, cool, awesome, symmetry, amazing, creative\n[Fooocus] Encoding positive #1 ...\n[Fooocus Model Management] Moving model(s) has taken 0.12 seconds\n[Fooocus] Encoding positive #2 ...\n[Fooocus] Encoding negative #1 ...\n[Fooocus] Encoding negative #2 ...\n[Parameters] Denoising Strength = 1.0\n[Parameters] Initial Latent shape: Image Space (896, 1152)\nPreparation time: 2.26 seconds\n[Sampler] refiner_swap_method = joint\n[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\nRequested to load SDXL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 2.40 seconds\n100%|███████████████████████████████████████████| 30/30 [00:27<00:00,  1.08it/s]\nRequested to load AutoencoderKL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 0.29 seconds\nImage generated with private log at: /kaggle/working/Fooocus/outputs/2024-01-04/log.html\nGenerating and saving time: 33.38 seconds\n[Sampler] refiner_swap_method = joint\n[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\nRequested to load SDXL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 1.77 seconds\n100%|███████████████████████████████████████████| 30/30 [00:28<00:00,  1.06it/s]\nRequested to load AutoencoderKL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 0.30 seconds\nImage generated with private log at: /kaggle/working/Fooocus/outputs/2024-01-04/log.html\nGenerating and saving time: 33.26 seconds\nRequested to load SDXLClipModel\nRequested to load GPT2LMHeadModel\nLoading 2 new models\n[Fooocus Model Management] Moving model(s) has taken 0.79 seconds\nTotal time: 74.20 seconds\n[Parameters] Adaptive CFG = 7\n[Parameters] Sharpness = 2\n[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n[Parameters] CFG = 4.0\n[Parameters] Seed = 5887525531431292827\n[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n[Parameters] Steps = 30 - 15\n[Fooocus] Initializing ...\n[Fooocus] Loading models ...\nRefiner unloaded.\n[Fooocus] Processing prompts ...\n[Fooocus] Preparing Fooocus text #1 ...\n[Prompt Expansion] two red head woman deep tongue kissing each other on the beach, full body, intricate, elegant, highly detailed, wonderful, cute, divine, dramatic, magnificent, sharp focus, extremely detail, professional still, stunning, amazing, attractive, delicate, smart, decorative, illuminated, elaborate, complex, brilliant, thought, cinematic, breathtaking, light, shining, polished, shiny\n[Fooocus] Preparing Fooocus text #2 ...\n[Prompt Expansion] two red head woman deep tongue kissing each other on the beach, full body, intricate, elegant, highly detailed, lush, sharp focus, dramatic light, professional bright colors, cute, smart, enhanced, loving, attractive, modern, inspired, cinematic, fine, sublime, epic, directed, rich, color, extremely, determined, innocent, vibrant, breathtaking, symmetry\n[Fooocus] Encoding positive #1 ...\n[Fooocus Model Management] Moving model(s) has taken 0.12 seconds\n[Fooocus] Encoding positive #2 ...\n[Fooocus] Encoding negative #1 ...\n[Fooocus] Encoding negative #2 ...\n[Parameters] Denoising Strength = 1.0\n[Parameters] Initial Latent shape: Image Space (896, 1152)\nPreparation time: 2.19 seconds\n[Sampler] refiner_swap_method = joint\n[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\nRequested to load SDXL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 2.42 seconds\n100%|███████████████████████████████████████████| 30/30 [00:27<00:00,  1.09it/s]\nRequested to load AutoencoderKL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 0.30 seconds\nImage generated with private log at: /kaggle/working/Fooocus/outputs/2024-01-04/log.html\nGenerating and saving time: 33.38 seconds\n[Sampler] refiner_swap_method = joint\n[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\nRequested to load SDXL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 1.78 seconds\n100%|███████████████████████████████████████████| 30/30 [00:28<00:00,  1.06it/s]\nRequested to load AutoencoderKL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 0.30 seconds\nImage generated with private log at: /kaggle/working/Fooocus/outputs/2024-01-04/log.html\nGenerating and saving time: 33.39 seconds\nRequested to load SDXLClipModel\nRequested to load GPT2LMHeadModel\nLoading 2 new models\n[Fooocus Model Management] Moving model(s) has taken 0.79 seconds\nTotal time: 74.75 seconds\n[Parameters] Adaptive CFG = 7\n[Parameters] Sharpness = 2\n[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n[Parameters] CFG = 4.0\n[Parameters] Seed = 3972628851705127833\n[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n[Parameters] Steps = 30 - 15\n[Fooocus] Initializing ...\n[Fooocus] Loading models ...\nRefiner unloaded.\n[Fooocus] Processing prompts ...\n[Fooocus] Preparing Fooocus text #1 ...\n[Prompt Expansion] two pretten girls kissing, detailed, highly coherent, sharp focus, ambient light, dynamic background, adventurous, intricate, elegant, color, inspired, new, shiny, colorful, surreal, amazing, daring, creative, cinematic, stunning, beautiful, illuminated, pretty, attractive, smart, enhanced, famous, best, cool, awesome, cute, nice, charming, peaceful\n[Fooocus] Preparing Fooocus text #2 ...\n[Prompt Expansion] two pretten girls kissing, detailed colorful, sharp focus, highly coherent, elegant, symmetry, full color, cinematic, unique, cute, innocent, novel, cheerful, pretty, artistic, striking, fine detail, light, iconic, fantastic, amazing, pure glowing colors, beautiful, creative, wonderful, positive, adventurous, vibrant, intricate, attractive, cool, enhanced\n[Fooocus] Encoding positive #1 ...\n[Fooocus Model Management] Moving model(s) has taken 0.12 seconds\n[Fooocus] Encoding positive #2 ...\n[Fooocus] Encoding negative #1 ...\n[Fooocus] Encoding negative #2 ...\n[Parameters] Denoising Strength = 1.0\n[Parameters] Initial Latent shape: Image Space (896, 1152)\nPreparation time: 2.42 seconds\n[Sampler] refiner_swap_method = joint\n[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\nRequested to load SDXL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 2.50 seconds\n100%|███████████████████████████████████████████| 30/30 [00:27<00:00,  1.08it/s]\nRequested to load AutoencoderKL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 0.31 seconds\nImage generated with private log at: /kaggle/working/Fooocus/outputs/2024-01-04/log.html\nGenerating and saving time: 33.79 seconds\n[Sampler] refiner_swap_method = joint\n[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\nRequested to load SDXL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 1.75 seconds\n  0%|                                                    | 0/30 [00:00<?, ?it/s]\nUser skipped\nRequested to load SDXLClipModel\nRequested to load GPT2LMHeadModel\nLoading 2 new models\n[Fooocus Model Management] Moving model(s) has taken 0.80 seconds\nTotal time: 40.54 seconds\n[Parameters] Adaptive CFG = 7\n[Parameters] Sharpness = 2\n[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n[Parameters] CFG = 4.0\n[Parameters] Seed = 1955220653578221877\n[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n[Parameters] Steps = 30 - 15\n[Fooocus] Initializing ...\n[Fooocus] Loading models ...\nRefiner unloaded.\n[Fooocus] Processing prompts ...\n[Fooocus] Preparing Fooocus text #1 ...\n[Prompt Expansion] two pretten school girls kissing, full body, detailed intricate, light atmosphere, beautiful, striking, elaborate, highly saturated colors, artistic, sharp focus, elegant, fine detail, cinematic, complex, stunning, rich deep color, enhanced quality, very inspirational, cute, animated, professional, surreal, iconic, creative, expressive, positive, awesome, joyful, unique, cool\n[Fooocus] Preparing Fooocus text #2 ...\n[Prompt Expansion] two pretten school girls kissing, full body, detailed intricate, sharp focus, vivid colors, optimistic, magical, cinematic, highly enhanced, mystical, theatrical, rich deep color, sublime, extremely beautiful, epic, grand perfect, dramatic light, directed, coherent, symmetry, great composition, innocent, iconic, fine detail, polished, complex, amazing dynamic, lovely, colorful\n[Fooocus] Encoding positive #1 ...\n[Fooocus Model Management] Moving model(s) has taken 0.12 seconds\n[Fooocus] Encoding positive #2 ...\n[Fooocus] Encoding negative #1 ...\n[Fooocus] Encoding negative #2 ...\n[Parameters] Denoising Strength = 1.0\n[Parameters] Initial Latent shape: Image Space (896, 1152)\nPreparation time: 2.36 seconds\n[Sampler] refiner_swap_method = joint\n[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\nRequested to load SDXL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 2.37 seconds\n100%|███████████████████████████████████████████| 30/30 [00:30<00:00,  1.02s/it]\nRequested to load AutoencoderKL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 0.30 seconds\nImage generated with private log at: /kaggle/working/Fooocus/outputs/2024-01-04/log.html\nGenerating and saving time: 36.12 seconds\n[Sampler] refiner_swap_method = joint\n[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\nRequested to load SDXL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 1.78 seconds\n100%|███████████████████████████████████████████| 30/30 [00:27<00:00,  1.09it/s]\nRequested to load AutoencoderKL\nLoading 1 new model\n[Fooocus Model Management] Moving model(s) has taken 0.29 seconds\nImage generated with private log at: /kaggle/working/Fooocus/outputs/2024-01-04/log.html\nGenerating and saving time: 32.57 seconds\nRequested to load SDXLClipModel\nRequested to load GPT2LMHeadModel\nLoading 2 new models\n[Fooocus Model Management] Moving model(s) has taken 0.79 seconds\nTotal time: 76.57 seconds\n","output_type":"stream"}]}]}